{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E9 Planners and their future promise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.3.8.dev0\n",
    "!python -m pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.connectors.memory.chroma import ChromaMemoryStore\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI services used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", AzureTextEmbedding(\"text-embedding-ada-002\", api_key, endpoint))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-3.5-turbo-0301\", api_key, org_id))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", api_key, org_id))\n",
    "\n",
    "#kernel.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "kernel.register_memory_store(memory_store=ChromaMemoryStore(persist_directory='mymemories'))\n",
    "kernel.import_skill(sk.core_skills.TextMemorySkill())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's get a ðŸ”¥ kernel ready\n",
    "\n",
    "We're going to create a kernel that includes an embedding generation service â€” so that we can best construct the context that will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that in Example 2 I attempt to load the currently compiled SK locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 (skip to Example 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI services used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-3.5-turbo-0301\", api_key, org_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.core_skills import FileIOSkill, MathSkill, TextSkill, TimeSkill\n",
    "from semantic_kernel.planning import SequentialPlanner\n",
    "\n",
    "kernel.import_skill(MathSkill(), \"math\")\n",
    "kernel.import_skill(FileIOSkill(), \"fileIO\")\n",
    "kernel.import_skill(TimeSkill(), \"time\")\n",
    "kernel.import_skill(TextSkill(), \"text\")\n",
    "\n",
    "# create an instance of sequential planner.\n",
    "planner = SequentialPlanner(kernel)\n",
    "\n",
    "# the ask for which the sequential planner is going to find a relevant function.\n",
    "ask = \"What day of the week is today, all uppercase?\"\n",
    "\n",
    "# ask the sequential planner to identify a suitable function from the list of functions available.\n",
    "plan = await planner.create_plan_async(goal=ask)\n",
    "\n",
    "print(plan.description)\n",
    "\n",
    "# ask the sequential planner to execute the identified function.\n",
    "result = await plan.invoke_async()\n",
    "\n",
    "for step in plan._steps:\n",
    "    print(step.description, \":\", step.skill_name, step._function.name, step._state.__dict__)\n",
    "\n",
    "print(\"Expected Answer:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI services used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "#    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-3.5-turbo\", api_key, org_id))\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-3.5-turbo-0301\", api_key, org_id))\n",
    "#    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-4\", api_key, org_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.planning import SequentialPlanner\n",
    "from semantic_kernel.core_skills.text_skill import TextSkill\n",
    "\n",
    "skills_directory = \"./skills\"\n",
    "summarize_skill = kernel.import_semantic_skill_from_directory(skills_directory, \"SummarizeSkill\")\n",
    "writer_skill = kernel.import_semantic_skill_from_directory(skills_directory, \"WriterSkill\")\n",
    "#text_skill = kernel.import_skill(TextSkill(), \"TextSkill\")\n",
    "\n",
    "# create an instance of sequential planner.\n",
    "planner = SequentialPlanner(kernel)\n",
    "\n",
    "# the ask for which the sequential planner is going to find a relevant function.\n",
    "ask = \"\"\"\n",
    "Tomorrow is Valentine's day. I need to come up with a poem. Translate the poem to French.\"\"\"\n",
    "\n",
    "# ask the sequential planner to identify a suitable function from the list of functions available.\n",
    "plan = await planner.create_plan_async(goal=ask)\n",
    "\n",
    "# ask the sequential planner to execute the identified function.\n",
    "result = await plan.invoke_async()\n",
    "\n",
    "for step in plan._steps:\n",
    "    print(step.description, \":\", step.skill_name + \".\" + step._function.name, step._parameters._variables, step._outputs)\n",
    "\n",
    "for index, step in enumerate(plan._steps):\n",
    "    print(\"Step:\", index)\n",
    "    print(\"Description:\",step.description)\n",
    "    print(\"Function:\", step.skill_name + \".\" + step._function.name)\n",
    "    print(\"Input vars:\", step._parameters._variables)\n",
    "    print(\"Output vars:\", step._outputs)\n",
    "    if len(step._outputs) > 0:\n",
    "        print( \"  Output:\", str.replace(result[step._outputs[0]],\"\\n\", \"\\n        \"))\n",
    "\n",
    "    print()\n",
    "print(\"Final Answer:\")\n",
    "print(result)\n",
    "\n",
    "#for step in plan._steps:\n",
    "#    print(step.description, \":\", step.skill_name, step._function.name, step._state.__dict__)\n",
    "\n",
    "print(\"Expected Answer:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI services used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "#    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-3.5-turbo\", api_key, org_id))\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-3.5-turbo-0301\", api_key, org_id))\n",
    "#    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-4\", api_key, org_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from semantic_kernel.planning import SequentialPlanner\n",
    "from semantic_kernel.core_skills.text_skill import TextSkill\n",
    "\n",
    "skills_directory = \"./plugins-sk\"\n",
    "\n",
    "\n",
    "#dt_skill = kernel.import_semantic_skill_from_directory(skills_directory, \"DesignThinking\")\n",
    "bt_skill = kernel.import_semantic_skill_from_directory(skills_directory, \"BusinessThinking\")\n",
    "# text_skill = kernel.import_skill(TextSkill(), \"TextSkill\")\n",
    "\n",
    "# create an instance of sequential planner.\n",
    "planner = SequentialPlanner(kernel)\n",
    "\n",
    "# the ask for which the sequential planner is going to find a relevant function.\n",
    "# ask = \"\"\"\n",
    "# Tomorrow is Valentine's day. I need to come up with a few date ideas. She speaks French so write it in French.\n",
    "# Convert the text to uppercase.\"\"\"\n",
    "ask = \"\"\"\n",
    "I want to solve a business problem for a pizza shop owner who has a lot of customers who are unhappy.\"\"\"\n",
    "\n",
    "# ask the sequential planner to identify a suitable function from the list of functions available.\n",
    "plan = await planner.create_plan_async(goal=ask)\n",
    "\n",
    "# ask the sequential planner to execute the identified function.\n",
    "result = await plan.invoke_async()\n",
    "\n",
    "for step in plan._steps:\n",
    "    print(step.description, \":\", step.skill_name, step._function.name, step._state.__dict__)\n",
    "\n",
    "print(\"Expected Answer:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"\"\"\n",
    "I want to solve a business problem for a pizza shop owner who has a lot of customers who are unhappy.\"\"\"\n",
    "original_plan = await planner.create_plan_async(ask, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing the pizza shop owner is figure out how to get more time back for themself. Often times that time gained can be used for the pizza shop to reinvest in the same way that money saved can be re-leveraged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

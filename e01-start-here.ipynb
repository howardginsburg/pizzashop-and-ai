{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E1 Start Here\n",
    "\n",
    "First off you need to copy `env.example` to `.env` so that you can add your credentials. Be sure to always keep them safe. After you've done that, you can proceed to bringing the Semantic Kernel Python package over to your machine with the following command. This should stay persistent across you other workshop sessions, but if it doesn't then remember to re-do this within every notebook where you want to use Semantic Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.3.8.dev0\n",
    "!python -m pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did that work out okay? Great! Now, assuming that you added the key information to your `.env` file, the following should verify that you're ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Get a kernel ready\n",
    "\n",
    "Semantic Kernel requires a \"kernel\" object to process requests that are packaged as functions. There are either \"native\" functions written in regular computer code, or else there are \"semantic\" functions that are written as templated prompts. You fire up a kernel with a specific preference of model as usually an OpenAI or Azure OpenAI model, but Hugging Face models are also available to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-3.5-turbo-0301\", api_key, org_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÇ Create a semantic function inline in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Kernel uses a templating format where variables are denoted with `{{` and `}}` and a dollar sign before the name. The default variable name is `$input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prompt = \"\"\"\n",
    "{{$input}}\n",
    "\n",
    "Summarize the content above in less than 140 characters.\n",
    "\"\"\"\n",
    "\n",
    "print(\"A string has been set to be used as a semantic function.\")\n",
    "\n",
    "summary_function = kernel.create_semantic_function(prompt_template=sk_prompt,\n",
    "                                                    description=\"Summarizing prompt.\",\n",
    "                                                    max_tokens=200,\n",
    "                                                    temperature=0.1,\n",
    "                                                    top_p=0.5)\n",
    "\n",
    "print(\"A semantic function has been registered.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèéÔ∏è Set the default variable $input and run the function with SK\n",
    "\n",
    "Note that because `$input` is the default variable name, you don't have to specify it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_input = \"\"\"\n",
    "Let me illustrate an example. Many weekends, I drive a few minutes from my house to a local pizza store to buy \n",
    "a slice of Hawaiian pizza from the gentleman that owns this pizza store. And his pizza is great, but he always \n",
    "has a lot of cold pizzas sitting around, and every weekend some different flavor of pizza is out of stock. \n",
    "But when I watch him operate his store, I get excited, because by selling pizza, he is generating data. \n",
    "And this is data that he can take advantage of if he had access to AI.\n",
    "\n",
    "AI systems are good at spotting patterns when given access to the right data, and perhaps an AI system could spot \n",
    "if Mediterranean pizzas sell really well on a Friday night, maybe it could suggest to him to make more of it on a \n",
    "Friday afternoon. Now you might say to me, \"Hey, Andrew, this is a small pizza store. What's the big deal?\" And I \n",
    "say, to the gentleman that owns this pizza store, something that could help him improve his revenues by a few \n",
    "thousand dollars a year, that will be a huge deal to him.\n",
    "\"\"\";\n",
    "# Text source: https://www.ted.com/talks/andrew_ng_how_ai_could_empower_any_business/transcript\n",
    "\n",
    "summary = summary_function(sk_input)\n",
    "\n",
    "display(Markdown(str(summary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see a summary? Great! That means you're able to get cooking with the Semantic Kernel. Let's get going immediately because the pizza shop owner might really need our help!\n",
    "\n",
    "## ‚Åá Who's the pizza shop owner?\n",
    "\n",
    "[![](assets/andrew_ng.jpg)](assets/thepizzastore720.mp4)\n",
    "\n",
    "In his TED 2022 talk, Andrew Ng describes \"How AI could empower any business ([TED 2022](https://www.ted.com/talks/andrew_ng_how_ai_could_empower_any_business))\" as  means to help us all understand that the new kind of AI technology could be put into service for anybody ‚Äî including Andrew's favorite pizza shop.\n",
    "\n",
    "There are a variety of business strategy questions that can be asked with LLMs, and so we'll use Semantic Kernel to work our way towards repeatable processes that can be used for other businesses in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

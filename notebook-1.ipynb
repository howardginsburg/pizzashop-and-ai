{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßë‚Äçüç≥ Semantic Kernel is like\u000byour AI cooking kitchen\n",
    "\n",
    "First off you need to copy `env.example` to `.env` so that you can add your credentials. Be sure to always keep them safe. After you've done that, you can proceed to bringing the Semantic Kernel Python package over to your machine with the following command. This should stay persistent across you other workshop sessions, but if it doesn't then remember to re-do this within every notebook where you want to use Semantic Kernel.\n",
    "\n",
    "Next you need to install the necessary python packages, including semantic kernel and chromadb, for the samples.  If you're running this from GitHub Codespaces, we already run this for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did that work out okay? Great! Now, assuming that you added the key information to your `.env` file, the following should verify that you're ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Get a kernel ready\n",
    "\n",
    "Semantic Kernel requires a \"kernel\" object to process requests that are packaged as functions. There are either \"native\" functions written in regular computer code, or else there are \"semantic\" functions that are written as templated prompts. You fire up a kernel with a specific preference of model as usually an OpenAI or Azure OpenAI model, but Hugging Face models are also available to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = True\n",
    "\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"azureopenai\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"openai\", OpenAIChatCompletion(\"gpt-3.5-turbo-0301\", api_key, org_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the event you want to use an open-source Hugging Face model, it would look like the following ‚Äî which will require installation of related packages and a bit of time for the models to download and get stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "kernel.add_text_completion_service(\"huggingface\", HuggingFaceTextCompletion(\"gpt2\", task=\"text-generation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîñ Reminder: A kernel is like the stove in a kitchen. It doesn't do anything unless you start to cook with it. Let's proceed to get it a few functions to üî• cook up for us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
